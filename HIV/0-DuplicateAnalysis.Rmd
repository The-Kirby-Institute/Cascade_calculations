---
title: "Exploration of duplicates in HIV notifications"
author: "Richard T. Gray"
date: '`r format(Sys.Date(), format="%d %B %Y")`'
output: 
  word_document:
    pandoc_args: --output="docs/Duplicate_analysis.docx"
---

This Rmarkdown document describes an exploration of the number and 
proportion of duplicate notifications in the Australian HIV Registry. The 
primary reason for this exploration is to assess the validity of using the
deduplication algorithm published in:

> Law, M G, A M McDonald, and J M Kaldor. "Estimation of Cumulative HIV 
> Incidence in Australia, Based on National Case Reporting." Australian 
> and New Zealand Journal of Public Health 20, no. 2 (April 1996): 215-7.

for use in the Australian HIV diagnosis and care cascades. 

```{r knitr_options, include=FALSE} 
knitr::opts_chunk$set(echo = FALSE, 
  warning = FALSE, 
  message = FALSE, 
  include = FALSE) 
```

## Settings and assumptions

We applied the deduplication algorithm to the entire set of notifications 
in the HIV registry to the end of 2016 and separately for notifications 
from the states of New South Wales (NSW) and Victoria (Vic). For these
settings we estimated annual and cumulative notifications (including 
duplicates), the cumulative number and proportion of notifications that 
are unique, and the annual number and proportion of notifications that are
unique each year. 

The algorithm is statistical in nature and is applied to all notifications
since the start of Australia's HIV epidemic in 1980 (year of first 
notification). Annual estimates for a given year are obtained by applying 
this algorithm to all the notifications between 1980 and each given year 
and then taking the difference. This can lead to invalid estimates where 
the annual proportion unique is > 1 and the number of duplicates is 
negative. However, adding the annual number of duplicates will produce the
correct cumulative number of unique notifications and this value is the 
key estimate for the HIV cascades. Thus we pretend the annual estimates 
are valid for the purposes of our calculations and estimates. 

In recent years there is likely to be very few duplicates in the registry 
due to validation processes as part of HIV surveillance especially 
following an audit of all notifications in 2002 (I believe). So far we 
have simply applied the algorithm to all notifications and assumed the 
estimates are valid, even if the number of duplicates seems a bit high in 
recent years, and assumed other (uncertain) components of the cascade 
calculations (such as emigration) balance out any errors in the estimates 
for duplicates. We explored the validity of this assumption by considering
scenarios where all notifications since 1992 or 2002 overall and for each 
state are unique (arbitrarily chosen years). We then compared the 
estimates for each setting and scenario.

```{r Initilization}
# Open as a project (setting working directory to source and restarting R)

# Setup directories
basePath <- getwd()
Rcode <- file.path(dirname(basePath), "code") # All cascades code
HIVcode <- file.path(basePath, "code") 
dataFolder <- file.path(basePath, "data")
resultsFolder <- file.path(basePath, "output")
figuresFolder <- file.path(basePath, "output", "figures")

# Notifications folder is a private secure folder 
notificationsFolder <- file.path("/", "SVR-NAS", "Public", "SERP", "Data", 
  "National HIV Registry", "2017", "National", "Hamish & Richard")

# Load libraries and standard functions
source(file.path(Rcode, "LoadLibrary.R"))
LoadLibrary(tidyverse)
LoadLibrary(cowplot)
LoadLibrary(captioner)
LoadLibrary(scales)
source(file.path(Rcode, "FormatData.R"))

# Script options
savePlots <- FALSE
dataYear <- 2017
analysisYear <- 2016

# It can take a while to generate the results so create the option to just
# save and load the results.
recreateResults <- FALSE # If false load from "DuplicateAnalysis.Rda" in
                        # results folder 

# Captions setup
figs <- captioner()
tabs <- captioner(prefix = "Table")

```

```{r Load and clean notifications}
# This chunk loads the national notifications and subsets based on the
# specified criteria in Script paramaters

# Function to tidy notifications
source(file.path(HIVcode, "TidyNotifications.R"))

# Load cleaned notifications data
origHivData <- read_csv(file.path(notificationsFolder,
  paste0("cascadeHIVnotifications-clean-", toString(dataYear), ".csv"))) 

# Read in country code data
countryCodes <- read_csv(file.path(dataFolder, "countryRegionCodes.csv")) 

# Tidy up notifications for calculations
hivData <- TidyNotifications(origHivData, analysisYear, countryCodes) %>%
  filter(yearhiv <= analysisYear)


# Data checks-------------------------------------------------------------

# Quick check that we are not doing analysis outside of the data
if (max(hivData$yeardiagnosis) < analysisYear) {
  stop("No data specified for final year of analysis.")
}

# HIV data variables------------------------------------------------------

# Setup variables for overarching analysis                  
allYears <- min(hivData$yeardiagnosis):analysisYear # all years of data

```

```{r Generate results}
# This chunk generates the estimates for duplicates for the key 
# sub-populations we are investigating. 
# recreateResults <- TRUE # within chunk change

if (recreateResults) {
  hivSetAll <- filter(hivData, yeardiagnosis <= analysisYear)
  
  # Get our subsets ------------------------------------------------------
  # Extract subset of notifications we want using function to easily
  # extract sub-populations of interest 
  source(file.path(HIVcode, "SubHivSet.R"))
  
  # SubHivSet(hivSetAll, targetAge, targetGender, targetExposure,
  #   targetCob, targetAtsi, targetState, targetGlobalRegion)
  allHiv <- SubHivSet(hivSetAll, "all", "all", "all", "all", "all", "all",
    "all")[[1]]
  maleHiv <- SubHivSet(hivSetAll, "all", "male", "all", "all", "all",
    "all", "all")[[1]]
  femaleHiv <- SubHivSet(hivSetAll, "all", "female", "all", "all", "all",
    "all", "all")[[1]]
  
  # Males by exposure group
  msmHiv <- hivSetAll %>%
    filter(sex == "male", expgroup == "msm")
  heteroHiv <- hivSetAll %>%
    filter(sex == "male", expgroup == "hetero")
  pwidHiv <- hivSetAll %>%
    filter(sex == "male", expgroup == "pwid")
  unknownHiv <- hivSetAll %>%
    filter(sex == "male", expgroup == "unknown")
  otherHiv <- hivSetAll %>%
    filter(sex == "male", expgroup == "otherexp")
  comboHiv1 <- hivSetAll %>%
    filter(sex == "male", expgroup %in% c("msm", "unknown"))
  comboHiv2 <- hivSetAll %>%
    filter(sex == "male", expgroup %in% c("pwid", "unknown"))
  
  # By state
  nswHiv <- SubHivSet(hivSetAll, "all", "all", "all", "all", "all", "nsw",
    "all")[[1]]
  vicHiv <- SubHivSet(hivSetAll, "all", "all", "all", "all", "all", "vic",
    "all")[[1]]
  
  nonNswHiv <- SubHivSet(hivSetAll, "all", "all", "all", "all", "all",
    "nsw", "all")[[2]]
  nswMaleHiv <- SubHivSet(hivSetAll, "all", "male", "all", "all", "all",
    "nsw", "all")[[1]]
  nonNswMaleHiv <- SubHivSet(maleHiv, "all", "all", "all", "all",
    "all", "nsw", "all")[[2]]

  # Generate results we want ---------------------------------------------
  # Functions for removing duplicates annually
  source(file.path(Rcode, "FillMissing.R"))
  source(file.path(Rcode, "FillDataFrame.R"))
  source(file.path(HIVcode, "DeduplicationFunctions.R"))
  
  all2022 <- GetUnique(allHiv, allYears) %>%
    mutate(population = "all", replace = "2022")
  
  # By sex
  male2022 <- GetUnique(maleHiv, allYears) %>%
    mutate(population = "male", replace = "2022")
  female2022 <- GetUnique(femaleHiv, allYears) %>%
    mutate(population = "female", replace = "2022")
  
  # Male by exposure group
  msm2022 <- GetUnique(msmHiv, allYears) %>%
    mutate(population = "msm", replace = "2022")
  hetero2022 <- GetUnique(heteroHiv, allYears) %>%
    mutate(population = "heteromale", replace = "2022")
  pwid2022 <- GetUnique(pwidHiv, allYears) %>%
    mutate(population = "pwidmale", replace = "2022")
  unknown2022 <- GetUnique(unknownHiv, allYears) %>%
    mutate(population = "unknownmale", replace = "2022")
  other2022 <- GetUnique(otherHiv, allYears) %>%
    mutate(population = "othermale", replace = "2022")
  combo12022 <- GetUnique(comboHiv1, allYears) %>%
    mutate(population = "msmunknown", replace = "2022")
  combo22022 <- GetUnique(comboHiv2, allYears) %>%
    mutate(population = "msmidu", replace = "2022")
  
  # compare overall proportion to summed proportion
  weightedProp <- (msm2022$totalnotifications * msm2022$cumunique + 
    hetero2022$totalnotifications * hetero2022$cumunique +
    pwid2022$totalnotifications * pwid2022$cumunique +
    unknown2022$totalnotifications * unknown2022$cumunique +
    other2022$totalnotifications * other2022$cumunique) /
    male2022$totalnotifications
  
  # Unique scenarios
  all2002 <- GetUnique(allHiv, allYears, yearUnique = 2002) %>%
    mutate(population = "all", replace = "2002")
  all1992 <- GetUnique(allHiv, allYears, yearUnique = 1992) %>%
    mutate(population = "all", replace = "1992")
  
  nsw2022 <- GetUnique(nswHiv, allYears) %>%
    mutate(population = "nsw", replace = "2022")
  nsw2002 <- GetUnique(nswHiv, allYears, yearUnique = 2002) %>%
    mutate(population = "nsw", replace = "2002")
  nsw1992 <- GetUnique(nswHiv, allYears, yearUnique = 1992) %>%
    mutate(population = "nsw", replace = "1992")
  
  vic2022 <- GetUnique(vicHiv, allYears) %>%
    mutate(population = "vic", replace = "2022")
  vic2002 <- GetUnique(vicHiv, allYears, yearUnique = 2002) %>%
    mutate(population = "vic", replace = "2002")
  vic1992 <- GetUnique(vicHiv, allYears, yearUnique = 1992) %>%
    mutate(population = "vic", replace = "1992")
  
  nonnsw2022 <- GetUnique(nonNswHiv, allYears) %>%
    mutate(population = "nonnsw", replace = "2022")
  nswMale2022 <- GetUnique(nswMaleHiv, allYears) %>%
    mutate(population = "nswmale", replace = "2022")
  nonnswmale2022 <- GetUnique(nonNswMaleHiv, allYears) %>%
    mutate(population = "nonnswmale", replace = "2022")
  
  # Put everything into a giant data frame
  duplicateResults <- bind_rows(all2022, male2022, female2022, msm2022,
    hetero2022, pwid2022, unknown2022, other2022, combo12022, combo22022, 
    all2002, all1992, nsw2022, nsw2002, nsw1992, vic2022,
    vic2002, vic1992, nonnsw2022, nswMale2022, nonnswmale2022, 
    .id = "set") %>%
    select(set, year, population, replace, everything())
  
  # Save results for reloading
  maleunique <- male2022$cumunique
  save(duplicateResults, weightedProp, maleunique, all2022,
    file = file.path(resultsFolder,
    "DuplicateAnalysis.Rda"))
} else {
  # Load previoulsly generated file
  load(file = file.path(resultsFolder,
    "DuplicateAnalysis.Rda"))
}
```

\pagebreak

## Results

```{r Plot results}
# This chuck produces all the plots.
source(file.path(Rcode, "PlotOptions.R"))
source(file.path(Rcode, "PlotColors.R"))
# source(file.path(Rcode, "GetLegend.R"))
source(file.path(Rcode, "SaveFigure.R"))

# Setup plot specifications ----------------------------------------------
cols <- PlotColors("main")

scenarios <- c("total", "cum1992", "cum2002", "cumall")
scenarioNames <- c("total" = "All notifications",
  "cum2002" = "Assumed unique since 2002",
  "cum1992" = "Assumed unique since 1992",
  "cumall" = "Unique notifications")
scenarioCols <- cols[1:4]
names(scenarioCols) <- scenarios

plotOpts <- PlotOptions() + theme(legend.position = "right",
    plot.title = element_text(hjust = 0.5))

# Function for plotting results for cumulative notifications
resultsPlot <- function(data) {
  
  plot <- ggplot(data = filter(data, replace == 2022), 
    aes(x = year)) +
    geom_line(aes(y = totalnotifications, colour = "total")) +
    geom_line(data = filter(data, replace == 1992),
      aes(y = cum_unique_replace, colour = "cum1992")) +
    geom_line(data = filter(data, replace == 2002),
      aes(y = cum_unique_replace, colour = "cum2002")) +
    geom_line(aes(y = cum_unique, colour = "cumall")) +
    scale_colour_manual(name = "", 
      limits = scenarios,
      breaks = scenarios,
      labels = scenarioNames[scenarios],
      values = scenarioCols[scenarios]) +
    scale_y_continuous(label = comma) + 
    labs(x = "Year", y = "Cumulative\nnotifications") + 
    plotOpts
  
  return(plot)
}

# Figure 1 ---------------------------------------------------------------
# Figure comparing national, nsw, and victorian notifications

allResults <-resultsPlot(filter(duplicateResults, population == "all")) +
    labs(title = "National notifications")
nswResults <-resultsPlot(filter(duplicateResults, population == "nsw")) +
    labs(title = "NSW notifications")
vicResults <-resultsPlot(filter(duplicateResults, population == "vic")) +
    labs(title = "Victoria notifications")

# Create a combined figure
figure1 <- plot_grid(allResults, nswResults, vicResults,
  labels = c("A", "B", "C", ""), ncol = 1, nrow = 3)

# Figure caption 
figs("figure1", "Cumulative total (black line) and unique notifications 
(green line) for A) Australia overall, B) NSW, and C) Victoria. The blue
  and red lines show the results when all notifications since 1992
  and 2002 are assumed to be unique, respectively.")

# Figure 2 ---------------------------------------------------------------

allResultsZoom <- allResults + 
 xlim(2010, analysisYear) + 
 scale_y_continuous(label = comma, limits = c(25000, NA))
nswResultsZoom <- nswResults + 
 xlim(2010, analysisYear) + 
 scale_y_continuous(label = comma, limits = c(15000, NA))
vicResultsZoom <- vicResults + 
 xlim(2010, analysisYear) + 
 scale_y_continuous(label = comma, limits = c(6000, NA))

# Create a combined figure
figure2 <- plot_grid(allResultsZoom, nswResultsZoom, vicResultsZoom,
  labels = c("A", "B", "C"), ncol = 1, nrow = 3)

# Figure caption 
figs("figure2", "Zoom in on results from Figure 1.")

# Figure 3 ---------------------------------------------------------------
# Figure 3a - Compare male and female cumulative poportion
propSexData <- filter(duplicateResults, 
  population %in% c("all", "male", "female"), replace == "2022")

sexs <- c("female", "male", "all")
sexNames <- c("female" = "Females", 
  "male" = "Males", 
  "all" = "Overall")
sexCols <- cols[1:3]
names(sexCols) <- sexs

propPlotSex <- ggplot(data = propSexData, aes(x = year, y = cumunique,
  colour = population)) + 
  geom_line() +
  scale_colour_manual(name = "Population", 
    breaks = sexs,
    limits = sexs,
    labels = sexNames[sexs],
    values = sexCols[sexs]) + 
  coord_cartesian(xlim = c(1986, analysisYear), ylim = c(0.75,1)) +
  scale_y_continuous(label = percent) +
  labs(y = "Cumulative proportion", x = "Year",
    title = "Proportion unique by sex") + 
  plotOpts

# Figure 3b - Compare state cumulative poportion
propStateData <- filter(duplicateResults, 
  population %in% c("all", "nsw", "vic"), replace == "2022")

states<- c("vic", "nsw", "all")
stateNames <- c("vic" = "Victoria", 
  "nsw" = "NSW", 
  "all" = "National")
stateCols <- cols[1:3]
names(stateCols) <- states

propPlotState <- ggplot(data = propStateData, aes(x = year, y = cumunique,
  colour = population)) + 
  geom_line() +
  scale_colour_manual(name = "Population", 
    breaks = states,
    limits = states,
    labels = stateNames[states],
    values = stateCols[states]) + 
  coord_cartesian(xlim = c(1986, analysisYear), ylim = c(0.75,1)) +
  scale_y_continuous(label = percent) +
  labs(y = "Cumulative proportion", x = "Year", 
    title = "Proportion unique by state") + 
  plotOpts

# Create a combined figure
figure3 <- plot_grid(propPlotSex, propPlotState,
  labels = c("A", "B"), ncol = 1, nrow = 2)

# Figure caption 
figs("figure3", "Cumulative proportion unique by sex and state.")

# Figure 4 ---------------------------------------------------------------
# Compare NSW to non-NSW data 
propPopData <- filter(duplicateResults, 
  population %in% c("all", "male", "nsw", "nswmale", "nonnsw",
    "nonnswmale"), replace == "2022")

pops <- c("nonnswmale", "nonnsw", "nswmale", "nsw", "male", "all") 
popNames <- c(
  "nonnswmale" = "Males outside of NSW",
  "nonnsw" = "Outside of NSW",
  "nswmale" = "NSW males",
  "nsw" = "NSW", 
  "male" = "Males", 
  "all" = "Overall")
popCols <- cols[1:6]
names(popCols) <- pops

figure4 <- ggplot(data = propPopData, aes(x = year, y = cumunique,
  colour = population)) + 
  geom_line() +
  scale_colour_manual(name = "Population", 
    breaks = pops,
    limits = pops,
    labels = popNames[pops],
    values = popCols[pops]) + 
  coord_cartesian(xlim = c(1986, analysisYear), ylim = c(0.75,1)) +
  scale_y_continuous(label = percent) +
  labs(y = "Cumulative proportion", x = "Year",
    title = "Proportion unique NSW vs rest of Australia") + 
  plotOpts

# Figure caption 
figs("figure4", "Cumulative proportion unique for NSW compared to rest of
  Australia.")

# Figure 5 ---------------------------------------------------------------
# Compare males by exposure
propMaleData <- filter(duplicateResults, 
  population %in% c("all", "male", "msm", "heteromale", "pwidmale", 
    "othermale", "unknownmale", "msmunknown", "msmidu"), 
  replace == "2022")

maleGroups <- c("othermale", "heteromale", "pwidmale", "msm", 
  "unknownmale", "msmidu", "msmunknown", "male", "all")
maleNames <- c("othermale" = "Males other exposure",
  "heteromale" = "Heterosexual males",
  "pwidmale" = "PWID males",
  "msm" = "Men who have sex with men",
  "unknownmale" = "Males unknown exposure",
  "msmidu" = "MSM + PWID",
  "msmunknown" = "MSM + unknown exposure",
  "male" = "All males", 
  "all" = "Overall")
maleCols <- cols[c(1:5, 7:10)]
names(maleCols) <- maleGroups

figure5A <- ggplot(data = propMaleData, aes(x = year, y = cum_unique,
  colour = population)) + 
  geom_line() +
  scale_colour_manual(name = "Population", 
    breaks = maleGroups,
    limits = maleGroups,
    labels = maleNames[maleGroups],
    values = maleCols[maleGroups]) + 
  coord_cartesian(xlim = c(1986, analysisYear)) +
  scale_y_continuous(label = comma) +
  labs(y = "Cumulative number", x = "Year",
    title = "Number unique") + 
  plotOpts

figure5B <- ggplot(data = propMaleData, aes(x = year, y = cumunique,
  colour = population)) + 
  geom_line() +
  scale_colour_manual(name = "Population", 
    breaks = maleGroups,
    limits = maleGroups,
    labels = maleNames[maleGroups],
    values = maleCols[maleGroups]) + 
  coord_cartesian(xlim = c(1986, analysisYear), ylim = c(0.85,1)) +
  scale_y_continuous(label = percent) +
  labs(y = "Cumulative proportion", x = "Year",
    title = "Proportion unique") + 
  plotOpts

# Create a combined figure
figure5 <- plot_grid(figure5A, figure5B,
  labels = c("A", "B"), ncol = 1, nrow = 2)

# Figure caption 
figs("figure5", "Cumulative number (A) and proportion (B) unique for male 
exposure category groups.")

# Save all the plots -----------------------------------------------------
if (savePlots) {
  SaveFigure(figuresFolder, "Duplicates_state_notifications", figure1,
    height = 17.5, width = 15, units = "cm")
  
  SaveFigure(figuresFolder, "Duplicates_zoom_notifications", figure2,
    height = 17.5, width = 15, units = "cm")
  
  SaveFigure(figuresFolder, "Duplicates_prop_unique", figure3,
    height = 15, width = 15, units = "cm")
  
  SaveFigure(figuresFolder, "Duplicates_prop_unique_test", figure4,
    height = 7.5, width = 15, units = "cm")
  
  SaveFigure(figuresFolder, "Duplicates_prop_unique_male", figure5,
    height = 15, width = 15, units = "cm")
  
}
```

The following figures show selected results for each population. Figures 1
and 2 show the change in cumulative notifications over time for each state
under various assumptions for when all annual notifications rare unique. 
Figure 1 shows the majority of duplicate notifications occurred prior to 
1992 and the number of duplicates from Victoria is small. Zooming in to 
recent years (Figure 2) shows there is little difference between the 
algorithm output and the assumption all notifications are unique since 
2002 (though the difference is a bit larger for NSW).   

`r figs("figure1")`
```{r Figure 1, include = TRUE, fig.width = 5, fig.height = 6}
figure1
```

\pagebreak

`r figs("figure2")`
```{r Figure 2, include = TRUE, fig.width = 6, fig.height = 7}
figure2
```

Figure 3 shows the cumulative proportion of notifications that are unique 
by sex and state. Over 95% of female and Overall Victorian notifications
are unique. The proportion in males, overall NSW, and overall nationally 
are very similar according to the algorithm.   

\pagebreak

`r figs("figure3")`
```{r Figure 3, include = TRUE, fig.width = 6, fig.height = 6}
figure3
```

I was still a little concerned there could be a saturation effect when 
applying the algorithm to large numbers of notifications. For example the 
proportion unique is very similar for national, male, and NSW 
notifications. Figure 4 compares the cumulative proportion unique in 
notifications in NSW and outside of NSW (with similar numbers of 
duplicates). This figure suggests the majority of duplicates occurred in
NSW males.  

`r figs("figure4")`
```{r Figure 4, include = TRUE, fig.width = 6, fig.height = 3}
figure4
```

Looking at male notifications further, I ran the the deduplication
algorithm on male notifications by exposure category. Figure 5 shows the
cumulative proportion unique for each male exposure group. Considering the
exposure groups separately results in an underestimate for the number of
duplicates with an overall cumulative percentage of `r FormatData(100*tail(weightedProp, 1), suffix = "%")` compared to the 
overall male percentage of `r FormatData(100*tail(maleunique,1), suffix = "%")`. This suggests duplicates are being missed because duplicate
notifications occur in distinct exposure categories (and hence are not
classified as duplicates when the algorithm is run on the separate exposure
categories). Figure 5 suggests combining the MSM and unknown exposure group
notifications produces a cumulative proportion unique that aligns with the 
overall male and national populations. 

`r figs("figure5")`
```{r Figure 5, include = TRUE, fig.width = 6, fig.height = 6}
figure5
```

## Discussion

* Using the overall proportion unique by applying the deduplication to all
notifications for males and the NSW population seems reasonable but 
separate calculations should be done for females and other states.
* For the 2017 ASR I did the deduplication calculations for males and 
females separately as required but the calculations should be done for 
each sub-population.
* Previously published estimates for the number unique in NSW obtained 
using the national estimates remain valid but NSW specific estimates will 
be generated in the future.
* Assuming all notifications since 2002 are unique will have little impact
on the HIV cascade estimates. 
* Seems like the majority of duplicates occurred in NSW males prior to 1992
particularly in MSM and males in the unknown exposure category (which makes
sense I think). 

## Appendix 1 - Annual duplicates and proportion unique

As stated previously when we estimated the number and proportion of 
duplicates annually we sometimes get a proportion > 1 and a negative number
of duplicates. This is an artifact of the statistical method and
calculation because we use the cumulative number to estimate annual 
duplicate numbers. Table 1 illustrates this using the national 
results from the last 10 years.

```{r Annual duplicates}
# This chunk produces plots for annual duplicates. This is primarily used 
# for illustration purposes so uses the national results from the last 10 
# years. 

LoadLibrary(knitr)

# Table of annual duplicates ----------------------------------------------
table1 <- all2022 %>%
  filter(year >= 2007) %>%
  select(year, totalnotifications, cumunique, cum_unique, unique, 
    notifications, duplicates, annunique) 

table1Str <- table1 %>%
  mutate(totalnotifications = FormatData(totalnotifications, places = 0), 
    cumunique = FormatData(cumunique, places = 3),
    cum_unique = FormatData(cum_unique, places = 1),
    unique = FormatData(unique, places = 1),
    notifications = FormatData(notifications),
    duplicates = FormatData(duplicates, places = 1),
    annunique = FormatData(annunique, places = 3))

colnames(table1Str) <- c("Year",
  "Cumulative notifications (A)",
  "Cumulative proportion unique (B)", 
  "Cumulative number unqiue (C = A*B)",
  "Number unique each year (D = diff C)",
  "Actual annual notifications (E)",
  "Annual number of duplicates (F=E-D)",
  "Annual proportion duplicates (G=F/D)")
  
tabs("Table1", "Annual numbers of duplicates for overall notifications. The
  letters and equations in brackets show how each column relates to the 
  other.")

# Figure 6 ---------------------------------------------------------------
# Plots of annual duplicates 
figure6A <- ggplot(data = table1, aes(x = year, y = duplicates)) + 
  geom_line(colour= cols["Blue"]) +
  geom_point(colour= cols["Blue"]) +
  geom_hline(yintercept = 0) +
  coord_cartesian(ylim = c(-150,150)) +
  ylab("Annual number") + xlab("Year") +
  PlotOptions()

figure6B <- ggplot(data = table1, aes(x = year, y = annunique)) + 
  geom_line(colour= cols["Blue"]) +
  geom_point(colour= cols["Blue"]) +
  geom_hline(yintercept = 1) +
  coord_cartesian(ylim = c(0.8, 1.2)) +
  ylab("Annual proportion") + xlab("Year") +
  PlotOptions()

# Create a combined figure
figure6 <- plot_grid(figure6A, figure6B,
  labels = c("A", "B"), ncol = 1, nrow = 2)

# Figure caption 
figs("figure6", "Number (A) and proportion (B) of annual  
  notifications that are unique for all national notifications.")

# Save all the plots -----------------------------------------------------
if (savePlots) {
  SaveFigure(figuresFolder, "Duplicates_annual_unique", figure5,
    height = 15, width = 10, units = "cm")
}

```

Even though proportions > 1 and negative number of duplicates don't make 
sense, I pretend they do in the cascade calculations as the cumulative
values do make sense and cumulative numbers are the important values in the
cascade calculations (you may get a small of error going from year to year
but that will be much smaller than the uncertainties in other aspects of 
the calculation).

Figure 6 shows the number and proportion of all annual notifications that 
are duplicates since 2007. The values tend to bounce around zero and 1 
respectively. Note the number of annual duplicates can be a far from zero 
(> 100) because the numbers are obtained by taking the difference of
cumulative notifications which are > 30,000. 

`r tabs("Table1")`

```{r Table 5 , include = TRUE, asis = "TRUE"}
kable(table1Str, row.names = FALSE)
```

`r figs("figure6")`

```{r Figure 6, include = TRUE, fig.width = 6, fig.height = 6}
figure6
```

## Appendix 2 - Cumulative duplicates analysis

To see if there is a natural time point from where we could assume all
notifications are unique I analysis the cumulative number of duplicates 
over 1980-2016 (shown in `r figs("figure7", display = "cite")`. This figure
suggests a rapid increase in duplicates until 1990 before a slow and steady 
increase since 1996. There has potentially been a stabilisation in 
duplicates since around 2005. In fact there is a fluctuation in cumulative 
diagnoses since 2007 which is unrealistic since cumulative duplicates 
should only increase.

```{r Analysis cumulative duplicates}
# Have a look at cumulative duplicates overtime to see if there is a point
# were we can assume duplicates no longer occur. We do this via
# visual inspection and by searching for breakpoints. 

# Overall cumulative duplicates in overall notifications
natResults <- filter(duplicateResults, population == "all", 
  replace == "2022")
natDuplicates <- natResults %>%
  select(year, cumduplicates)

# Look for breakpoints ---------------------------------------------------

# Load key libraries
LoadLibrary(tseries)
LoadLibrary(forecast)
LoadLibrary(strucchange)
LoadLibrary(ggfortify) # enable timeseries in autoplot
LoadLibrary(changepoint) # Not really used
LoadLibrary(segmented)
LoadLibrary(rsq) # load the rsq package for pseudo-R^2 calculation

# Covert everything into a time series which is more useful in the
# strucchange package functions. This means results are rounded to the
# nearest year
duplicatests <- ts(natDuplicates$cumduplicates,
  start = c(1980, 1), end = c(2016, 1),
  frequency = 1)
data <- cbind(duplicatests)
data <- window(data, start = c(1980, 1), end = c(2016, 1))
years <- 1980:2016

# Using the changepoint package. 
autoplot(cpt.mean(duplicatests))

# The following code looks for breakpoints in each indicator using the 
# breakpoints() function in the strucchange package.  

# Look for breakpoints - diagnoses
bpDuplicates <-breakpoints(duplicatests ~ years, data = data, h = 3)
summary(bpDuplicates)

autoplot(bpDuplicates)
plot(bpDuplicates)
plot(duplicatests)
lines(bpDuplicates)
# lines(confint(bpDuplicates)) # Produces an error
lines(fitted(bpDuplicates, breaks = 1), col = 4)
lines(fitted(bpDuplicates, breaks = 2), col = 2)
lines(fitted(bpDuplicates, breaks = 3), col = 3)

# Segmented changepoint analysis -----------------------------------------

# Specify initial breakpoints - use trial and error based on breakpoints()
# results and to test for sensitivity. 
minYear <- 1990 # 1980 for all
natDuplicatesZoom <- filter(natDuplicates, year >= minYear)
breakPoints <- c(1985, 1990, 2000)
breakPointsZoom <- breakPoints[breakPoints > minYear]

# Create a base model
trendDuplicates <- glm(round(cumduplicates) ~ year, data = natDuplicates, 
  family = "poisson")
trendDuplicatesZoom <- glm(round(cumduplicates) ~ year, 
  data = natDuplicatesZoom, 
  family = "poisson")

# Now run segmented to produce a segmented Poisson model. 
segDuplicates <- segmented(trendDuplicates, seg.Z = ~year, 
  psi = list(year = breakPoints))
segDuplicatesZoom <- segmented(trendDuplicatesZoom, seg.Z = ~year, 
  psi = list(year = breakPointsZoom))

segDuplicatesData <- data.frame(year = natDuplicates$year, 
  segdups = fitted(segDuplicates))
segDuplicatesDataZoom <- data.frame(year = natDuplicatesZoom$year, 
  segdups = fitted(segDuplicatesZoom))

# Plots of segmented changepoints -----------------------------------------

segCI <- function(est, stdErr) {
  return(c(est - 1.96 * stdErr, est + 1.96 * stdErr))
}

dupmax <- 3500

# Duplicates base plot
segDuplicatesPlotBase <- ggplot(data = natDuplicates, 
  aes(x = year, y = cumduplicates)) + 
  geom_line(aes(color = "estimates")) + 
  geom_point(color = cols["Red"]) +
  scale_x_continuous(breaks = seq(minYear,                        
    2016, by = 6)) +
  scale_y_continuous(labels = comma) + 
  coord_cartesian(xlim = c(1980, 2016), ylim = c(0, dupmax)) + 
  ylab("Cumulative duplicates") + xlab("Year") +  
  plotOpts 

# Duplicates only plot
duplicatesPlot <- segDuplicatesPlotBase + 
  theme(legend.position = "none")

# Overall duplicates plot
fullDuplicatesPlot <- segDuplicatesPlotBase + 
  geom_line(aes(y = fitted(trendDuplicates),
    colour = "overall")) +
  geom_line(data = segDuplicatesData, aes(y = segdups, 
    colour = "segment")) + 
  scale_colour_manual("", values = c(unname(cols["Red"]), "black", "blue"),
    limits = c("estimates", "overall", "segment"),
    labels = c("Estimates", "Overall model",
      "Segmented model")) +
  coord_cartesian(xlim = c(1980, 2016), ylim = c(0, dupmax)) + 
  ylab("Cumulative duplicates") + xlab("Year") +  
  plotOpts

# Zoomed duplicates plot
zoomDuplicatesPlot <- ggplot(data = natDuplicatesZoom, 
  aes(x = year, y = cumduplicates)) + 
  geom_line(aes(color = "estimates")) + 
  geom_point(color = cols["Red"]) +
  scale_x_continuous(breaks = seq(minYear,                        
    2016, by = 6)) +
  scale_y_continuous(labels = comma) + 
  coord_cartesian(xlim = c(1980, 2016), ylim = c(0, dupmax)) + 
  ylab("Cumulative duplicates") + xlab("Year") +  
  plotOpts  + 
  geom_line(aes(y = fitted(trendDuplicatesZoom),
    colour = "overall")) +
  geom_line(data = segDuplicatesDataZoom, aes(y = segdups, 
    colour = "segment")) + 
  scale_colour_manual("", values = c(unname(cols["Red"]), "black", "blue"),
    limits = c("estimates", "overall", "segment"),
    labels = c("Estimates", "Overall model",
      "Segmented model")) +
  coord_cartesian(xlim = c(minYear, 2016), ylim = c(0, dupmax)) + 
  ylab("Cumulative duplicates") + xlab("Year") +  
  plotOpts +
  geom_vline(xintercept = segDuplicatesZoom$psi[2], linetype = "dashed",
    colour = "blue") +
  geom_errorbarh(aes(y = 0.975 * dupmax,
    xmin = segCI(segDuplicatesZoom$psi[2],
      segDuplicatesZoom$psi[3])[1],
    xmax = segCI(segDuplicatesZoom$psi[2],
      segDuplicatesZoom$psi[3])[2]),
    colour = "blue",
    height = 0.025 * dupmax) +
  geom_text(x = 2007.5, y = 0.2 * dupmax,
    label = paste("Overall pseudo-R^2:",
      toString(round(rsq(trendDuplicatesZoom, type = "kl"), 
        digits = 4))), size = 3) +
  geom_text(x = 2007.5, y = 0.1 * dupmax,
    label = paste("Segmented pseudo-R^2:",
      toString(round(rsq(segDuplicates, type = "kl"), digits = 4))),
    size = 3)

# Summary: break points and slopes (zoom)
summary(segDuplicatesZoom)
segDuplicatesZoom$psi
segCI(segDuplicatesZoom$psi[2], segDuplicatesZoom$psi[3])
slope(segDuplicatesZoom)

# Combined plot ----------------------------------------------------------

# Create a combined figure
figure7 <- duplicatesPlot
  
figure8 <- plot_grid(fullDuplicatesPlot, zoomDuplicatesPlot, 
  labels = c("A", "B"), ncol = 1, nrow = 2)

# Figure captions 
figs("figure7", "Cumulative number of duplicate notifications over 
  time.")

figs("figure8", "(B) Trends in cumulative duplicates since 1980 with 
  segmented analysis. (C) Trend and segmented trend analysis since 1990 
  (trends recalculated from the truncated data set).")

```

`r figs("figure7")`

```{r Figure 7, include = TRUE, fig.width = 5, fig.height = 5}
figure7
```

To explore this further I performed a trend analysis including an 
exploration of change points. `r figs("figure8", display = "cite")` shows 
the results when the analysis is done on cumulative duplicates since
1980 and when cumulative duplicates are truncated from 1990. The trend in 
cumulative duplicates is well fitted by a segmented Poisson regression with 
a single change point in 1998. Since 1998 there is a steady increase in
cumulative duplicates at around 1% per year. This suggests there is no 
clear time where we can assume all duplicates are unique. However, given 
the fluctuation in cumulative diagnoses since 2007, potentially the 
de-duplication algorithm could be stopped at this point. 

`r figs("figure8")`

```{r Figure 8, include = TRUE, fig.width = 5, fig.height = 6}
figure8
```
